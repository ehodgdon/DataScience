---
title: "MovieLensProject"
author: "Ellis Hodgdon"
date: "2024-09-25"
output: html_document
bibliography: "MOvieLensProject.bib"
params:
  testing_set_percent: 0.10
  subset_percent: 0.10
  remove_files: FALSE
---

```{r libraries, echo=FALSE, include=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr")
```

```{r function definitions} 
regularization_function <- function(lambda, dataset){
  #calculating average for average training set raring
  mu <- mean(training_set$rating)
  
  #calculating movie bias for a given lambda
  b_sub_i <- training_set %>% 
    group_by(movieId) %>%
    summarize(b_sub_i = sum(rating - mu)/(n()+lambda))
 
  #calculating user bias for a given lambda
  b_sub_u <- training_set %>% 
    left_join(b_sub_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_sub_u = sum(rating - b_sub_i - mu)/(n()+lambda))
  
  # calculating genres bias for a given lambda
  b_genres <- training_set %>%
    left_join(b_sub_i, by="movieId") %>%
    left_join(b_sub_u, by="userId") %>%
    group_by(genres) %>%
    summarize(b_genres = sum(rating - b_sub_i - b_sub_u - mu)/(n()+lambda))
    

  #predicting ratings on test_set
  predicted_ratings <- 
    dataset %>% 
    left_join(b_sub_i, by = "movieId") %>%
    left_join(b_sub_u, by = "userId") %>%
    left_join(b_genres, by= "genres") %>%
    mutate(pred = mu + b_sub_i + b_sub_u + b_genres) %>%
    pull(pred)

  #calculating RMSE on test set for a given lambda
  return(RMSE(predicted_ratings, dataset$rating))
}
```

## Overview
#### Introduction
#### Problem Statement
#### RMSE
In the analysis of the scenario, we will be chasing the statistical quantity of Root Mean Square Error, or as mathematicians/statistians
prefer to call it, RMSE.It, and its close cousin, AME (Mean Absolute Error) have been embroiled in controversy for quite some time.
Which one is better? Willmott and Matsuura give arguments favoring one metric over the other, but, in reality, neither metric is inherently better [@Willmott2005]. RMSE is optimal for normal (Gaussian) errors while MAE is optimal of Laplacian errors. When the errors don't follow one of these patterns,there are other metrics out there that are better [@Hodson2022]. For this specific analysis, the RMSE will be used.  
A standard definition of RMSE is that it measures the average difference between a statistical model's predicted value and the actual values. Mathematically, it is the standard deviation of the residuals, which represent the distance between the regression line and the data points [@Frost]. It can be calculated ny the formula:  

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$



The calculation is not complicated and with a limited number of samples, can be done in Excel. However, when the number of samples gets large, as in this analysis, R has its RMSE function to do the calculations.


#### Process
## Method
The download and data wrangling code is provided by the course and is designed to join the ratings 
dataset with the movies dataset

#### Read external dataset
```{r location of external dataset}
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- "ml-10M100K.zip"
if (!file.exists(dl))     # do not download if file exists 
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
#
ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
# movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                            title = as.character(title),
#                                             genres = as.character(genres))

movies <- as.data.frame(str_split(read_lines(unzip(dl, "ml-10M100K/movies.dat")), fixed("::"), simplify=TRUE),            stringsAsFactors=FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies%>% mutate(movieId = as.integer(movieId))



movielens <- left_join(ratings, movies, by = "movieId")


```
#### Create validation (test) set
The testing portion will be `r format(100 * params$testing_set_percent, digits=2)` percent of the MovieLens data.This can be changed by modifying the parameters
```{r create validation set}

set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = params$testing_set_percent, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

if (params$remove_files) rm(dl, ratings, movies, test_index, temp, movielens, removed)

str(edx)
head(validation)


################################
# End of Provide Code
################################


################################
# Save files & load libraries
################################

save(edx, file = "edx.Rdata")
save(validation, file = "validation.Rdata")

```

#### Data Wrangling
```{r data wrangling & EDA Prep}
str(edx)

# Checking for missing values
sapply(edx, function(x) sum(is.na(x))) %>% 
  kable(col.names = c("Missing Values")) 

sapply(edx, function(x) sum(is.null(x))) %>% 
  kable(col.names = c("Missing Values")) 

# Unique movies and users
edxUniques <- summarize(edx, users = n_distinct(edx$userId), 
                        movies = n_distinct(edx$movieId)) %>%
                          kable(caption = "Table 1. Counts of Unique Users and Movies", col.names = c("Unique Users", "Unique Movies"))
edxUniques



# Transform dataset timestamp column to date
edx$timestamp <- edx$timestamp %>% as.POSIXct(origin = "1970-01-01")

# Create a year of release(assuming a mid-year release) and lag-to-rate columns
releaseYear <- as.Date(paste("07/01/",
                             str_sub(edx$title, str_length(edx$title) - 4,
                                     str_length(edx$title) - 1), sep=""), "%m/%d/%Y")
ratingLag <- as.numeric(as.Date(edx$timestamp) - releaseYear)
edx <- cbind(edx, ratingLag, releaseYear)
head(edx)


```
```{r update validation data set}

releaseYear <- as.Date(paste("07/01/",
                         str_sub(validation$title, str_length(validation$title) - 4, 
                         str_length(validation$title) -1), sep =""), "%m/%d/%Y")


validation <- cbind(validation, releaseYear)

  # Check for missing values
  sapply(validation, function(x) sum(is.na(x))) %>% 
    kable(col.names = c("Missing Values"))
  
  sapply(validation, function(x) sum(is.null(x))) %>% 
    kable(col.names = c("Missing Values")) 

  # Remove remaining extraneous objects
  if (params$remove_files) rm(ratingLag, releaseYear)
```

#### Data Sets
The data wrangling has produced two datasets: *edx* which will be used for training and *validation* which will be used to test the solution that is developed with the training set. For a first glimpse at the *edx* dataset:
```{r edx dataset}
head(edx, 10)
names(edx)
```
## Exploratory Data Analysis
Before any detailed analysis is started, we need to examine the data for any anomalies that might cause us problems later. We want to work with tidy data so we need to verify that the traing dataset, *edx*, is tidy. To be tidy, three conditions must be met:

* Each variable must have its own column
* Each observation must have its own row
* Each value must have its own cell



#### List of features
## Modeling 
#### Modeling Overview
Since we need to test the various models and should not use the validation dataset, we will partition the *edx* into
two sets, a training set and a test set. The percentage that is allocated to the testing_set is currently `r 100 * params$testing_set_percent` but can be changed via a parameter.
```{r build testing and training data sets}
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(edx$rating, times = 1, p = params$subset_percent, list = FALSE)
training_set <- data.frame(edx[-test_index,])
temp <- data.frame(edx[test_index ,])

testing_set <- temp %>%
  semi_join(training_set, by = "movieId") %>%
  semi_join(training_set, by = "userId")

# add back missing rows to the training set and remove extraneous data
withheld <- anti_join(temp, testing_set)
training_set <- rbind(training_set, withheld)
if (params$remove_files) rm (temp, withheld)
```


#### Model 0 - Naive model
This is a simple, naive  models which is predicting average movie rating for all observations. The formula  for this model is simply\

$$ Y_{} = \mu $$
```{r, model 0}
mu <- mean(training_set$rating)
mu
zero_rmse <- RMSE(testing_set$rating, mu)
zero_rmse

```

#### Model 1 - Average movie rating model

The formula for this model is\
$$ Y_{} = \mu + \epsilon $$

where $\mu$ is the mean of the data set (model 0), and $\epsilon$ is an error term that describes the random variability.

#### Calculations
```{r model 1 calculations}
first_avgs <- training_set %>% 
              group_by(movieId) %>% 
              summarize(epsilon = mean(rating - mu))
predicted_ratings <- mu + testing_set %>% 
                     left_join(first_avgs, by = "movieId") %>%
                     .$epsilon
                     
```                  
#### RMSE Results
We can now predict the rating with $\mu$, and we can obtain the RMSE for our first model:
```{r model 1 rmse results}
first_rmse <- RMSE(predicted_ratings, testing_set$rating)
first_rmse

```


#### Model 2 - Model 1 with movie effect model
We've got a RMSE for the average movie rating, but it is over 1 and our objectinve toto br below 0.8649, so we have a ways to go. To improve this RMSE, we will consider adding any effects that the movie may have on this value. There was a hint of thid effect when we noticed the more rstingd that a movie receives, the higher they often rated. We introduce a movie bias term, $\beta$, whichvis the difference between the average specific  movie$_i$ and thee average for all movies. Mathematically\
$$Y_{i} = \mu + \beta_{i} + \epsilon_{i}$$ \

##### Calculations
```{r second calculations}
second_avgs <- training_set %>% 
               left_join(first_avgs, by = "movieId") %>%
               group_by(userId) %>% 
               summarize(b_sub_i = mean(rating - mu - epsilon))
predicted_ratings <- testing_set %>% 
                     left_join(first_avgs, by='movieId') %>% 
                     left_join(second_avgs, by="userId") %>%
                     mutate(pred = mu + epsilon + b_sub_i) %>% 
                     .$pred
```

##### RMSE Results
```{r second rmse results}
second_rmse <- RMSE(predicted_ratings, testing_set$rating)
second_rmse
```
The RMSE is looking better. It is below 1 but still above our goal. Another model is inorder

#### Model 3 - Model 2 with user effect model
To the movie 2 model, we will add a user effect term. WE have noticed that a group of users the consistently give low ratings and there are those that consistently give higher ratings. Will use the symbol $b_u$ as this bias for user *u*. The formula now becomes:\
$$Y_{u,i} = \mu + \beta_{i} + b_u + \epsilon_{u,i}$$ \

#### Calculations
```{r third calculations}
third_avgs <- training_set %>% 
              left_join(first_avgs, by="movieId") %>%
              left_join(second_avgs, by='userId') %>% 
              group_by(releaseYear) %>% 
              summarize(b_genres = mean(rating - mu - epsilon - b_sub_i))
predicted_ratings <- testing_set %>% 
                     left_join(first_avgs, by="movieId") %>%
                     left_join(second_avgs, by='userId') %>%
                     left_join(third_avgs, by="releaseYear") %>%
                     mutate(pred = mu + epsilon + b_sub_i + b_genres) %>% 
                     .$pred
```

#### RMSE
The RMSE for this third model is
```{r}
third_rmse <- RMSE(predicted_ratings, testing_set$rating)
third_rmse
```
### Regularization of Model 3
This is better than before but still abovr the goal of 0.8469. To bring that value down below the goal, we will apply regularization thaat constraints to total variability of the various effects. Regularization is a set of methods for reducing overfitting in models such as this one. Typically, regularization exchanges a marginal decrease in training accuracy for an increase in generalizability [@MMuriel2023]. We implement regularization by include a new term lamda, $\lambda$ into ouyt equation
which now becomes\
$$\frac{1}{N} \sum_{u,i} (y_{u,i} = \mu - \beta_{i} - b_{i} - b_{u})^2 + \lambda (\sum_{i} + \beta_i^2 + \sum_{i} b_{u}^2)$$
where $\beta_{i}$ i influenced by movies that have jut a few ratings. $b_{u}$ is influenced by those users who only rated a small number of movies. The use of regularization allows us to penalize these effects. Consequently, we can use $\lambda$ as a tuning 
parameter that,by adjusting, allows us to minimize the RMSE.
```{r regularization}
lambdas <- c(seq(0, 4.5, 0.25), seq(4.6, 4.9, 0.1), seq(5.0, 10, 0.25))
rmses <- sapply(lambdas, regularization_function, testing_set)
```
```{r plot rmse results, fig.align="center", fig.width=6, fig.height=4}
# visualizing where lambda minimizes RMSE
min_lambda <- lambdas[which.min(rmses)]
plot_rmses <- ggplot() + 
              aes(x = lambdas, y = rmses) + 
              geom_point() + xlab("Lambda") + 
              ylab("RMSE") + ggtitle("Lambda Tuning") +
              theme(plot.title = element_text(hjust = 0.5)) +
              geom_vline(xintercept = min_lambda, color="blue")
plot_rmses
#finding which lambda minimizes RMSE
print(min_lambda)
```

                     

#### Summary of Results
```{r RMSE of validation dataset}
min_rmse <- regularization_function(min_lambda, testing_set)

validation_rmse <- regularization_function(min_lambda, validation)
validation_rmse
```
Using a $\lambda$ of `r format(min_lambda, digits=3)`, we determine a rmse of `r format(min_rmse, digits=6)`for the training set. It is now time for the grandiose test -- our algorithm against the validation dataset. Using the validation dataset, the rmse for that dataset is `r format(validation_rmse, digits=6)`. The goal has been met and exceeded.


#### Conclusion
For this project, we built a machine learning model that used the MovieLens Dataset to forecast movie ratings that takes into account any user movie bias, user bias, or genres bias. We initially used release year bias instead of genres bias, but were unable to meet the goal of the requirement. There are additional features in the dataset that could bve used to reduce the RMSE even further.