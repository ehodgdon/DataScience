---
title: "MovieLens Project"
author: "Ellis Hodgdon"
date: "Date: `r format(Sys.time(), '%a %d %b %Y')`"
output:
  pdf_document: 
    fig_height: 3
  html_document: default
bibliography: MovieLensProject.bib
params:
  dataset_selection: 1
  testing_set_percent: 0.1
  subset_percent: 0.1
  save_files: false
  large_dataset: false
  remove_obscure: true
  expand_genres: true
## index of datasets/movielens https://files.grouplens.org/datasets/movielens/
---

```{r setup and libraries, echo=FALSE, include=FALSE}
# names and urls of MovieLens datasets that have been tested
knitr::opts_chunk$set(echo = FALSE)
names <- list("ml-10m", "ml-32m", "ml-latest", "ml-latest-small",  "ml-25m")
urls <- list("ml-10m.zip", "ml-32m.zip", "ml-latest.zip", "ml-latest-small.zip", "ml-25m.zip")
folders <- list("ml-10M100K", "ml-32m", "ml-latest", "ml-latest-small", "ml-25m")

# load necessary libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("Extra", repos = "http://cran.us.r-project.org")
if(!require(magrittr)) install.packages("magrittr", repos = "http://cran.us.r-project.org")

```

```{r function definitions, echo=FALSE, include=FALSE} 
# ------------------------------------------------------------------------------------------------------
# collection of functions used 
# ------------------------------------------------------------------------------------------------------
# function to perform regularization on the given dataset for a specif lambda
regularization_function <- function(lambda, dataset){
  #calculating average for average training set raring
  mu <- mean(dataset$rating)
  
  #calculating movie bias for a given lambda
  b_movie <- dataset %>% 
    group_by(movieId) %>%
    summarize(b_movie = sum(rating - mu)/(n()+lambda))
 
  #calculating user bias for a given lambda
  b_user <- dataset %>% 
    left_join(b_movie, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_user = sum(rating - b_movie - mu)/(n()+lambda))
  
  # calculating genres bias for a given lambda
  b_genres <- dataset %>%
    left_join(b_movie, by="movieId") %>%
    left_join(b_user, by="userId") %>%
    group_by(genres) %>%
    summarize(b_genres = sum(rating - b_movie - b_user - mu)/(n()+lambda))
  
  # calculating release year bias for a given lambda
  b_year <- dataset %>%
    left_join(b_movie, by="movieId") %>%
    left_join(b_user, by="userId") %>%
    left_join(b_genres, by="genres") %>%
    group_by(releaseYear) %>%
    summarize(by = mean(rating - b_movie - b_user - b_genres - mu)/(n()+lambda))
   
    

  #predicting ratings on test_set
  predicted_ratings <- 
    dataset %>% 
    left_join(b_movie, by = "movieId") %>%
    left_join(b_user, by = "userId") %>%
    left_join(b_genres, by= "genres") %>%
    mutate(pred = mu + b_movie + b_user + b_genres) %>%
    pull(pred)

  #calculating RMSE on test set for a given lambda
  return(RMSE(predicted_ratings, dataset$rating))
}
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# check the dataset of NAs and/or nulls
check_for_nas_and_nulls <- function(dataset) {
cnt <- sapply(dataset, function(x) sum(is.na(x)))
if (sum(cnt) == 0)  {
  message("No NAs in dataset")
  } else {
  message("NAs in data set")
  cnt[cnt == 0] %>% kable(col.names = c("nulls present"))
  dataset <- na.omit(dataset)
  }
  
cnt <- sapply(dataset, function(x) sum(is.null(x)))
if (sum(cnt) == 0)  {
  message("No nulls/blanks in dataset")
  } else {
  cnt[cnt == 0] %>% kable(col.names = c("nulls present"))
  }
return (dataset)
}
# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# prepare dataset by converting timestamp, and adding release Year and ratingLag columns
dataset_prep <- function(dataset) {
# Transform dataset timestamp column to date
dataset$timestamp <- dataset$timestamp %>% as.POSIXct(origin = "1970-01-01")

dels <<- nrow(edx)
to_delete <- dataset[str_detect(dataset$title, "^.*\\(\\d{4}\\)$", negate=TRUE)]
dataset <- dataset[(!dataset$title %in% to_delete$title),]
dels <<- dels - nrow(edx)

# Create a year of release(assuming a mid-year release) and lag-to-rate columns
releaseYear <- as.Date(paste("07/01/",
                             str_sub(dataset$title, str_length(dataset$title) - 4,
                                     str_length(dataset$title) - 1), sep=""), "%m/%d/%Y")

ratingLag <- as.numeric(as.Date(dataset$timestamp) - releaseYear)
dataset <- dataset %>% cbind(ratingLag, releaseYear)  

check_for_nas_and_nulls(dataset) 

return(dataset)
}     # end of function

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# if selected, expand the dataset genres column into listing only single genre per row
expand_dataset <- function(dataset) {
  if (params$expand_genres) {
    dataset <- dataset %>% separate_rows(genres, sep = "\\|") %>% group_by(genres)
  }
  return (dataset)
}

# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# simple function to print the selectable parameters
show_parameters <- function() {
  testing_set_percent <- params$testing_set_percent
  subset_percent <- params$subset_percent
  save_files <- params$save_files
  large_dataset <- params$large_dataset
  remove_obscure <- params$remove_obscure
  expand_genres <- params$expand_genres
  parameters <- data.frame()
  parameters <- rbind(parameters, c("testing_set_percent", params$testing_set_percent))
  parameters <- rbind(parameters, c("subset_percent", params$subset_percent))
  parameters <- rbind(parameters, c("save_files", params$save_files))
  parameters <- rbind(parameters, c("large_dataset", params$large_dataset))
  parameters <- rbind(parameters, c("remove_obscure", params$remove_obscure))
  parameters <- rbind(parameters, c("expand_genres", params$expand_genres))
  parameters <- rbind(parameters, c("dataset name", names[params$dataset_selection]))
  colnames(parameters) <- c("Parameter", "Value")
  return (parameters)
  }
# ------------------------------------------------------------------------------------------------------
# end of functions
# ------------------------------------------------------------------------------------------------------
```

```{r start timer, echo=FALSE}
# keep track of elapsed time
start_time <- Sys.time()
```


## Overview
#### Problem Statement
In the analysis of the scenario, we will be chasing the statistical quantity of Root Mean Square Error, or as mathematicians/statisticians
prefer to call it, RMSE. It, and its close cousin, AME (Mean Absolute Error) have been embroiled in controversy for quite some time.
Which one is better? Willmott and Matsuura give arguments favoring one metric over the other, but, in reality, neither metric is inherently better [@Willmott2005]. RMSE is optimal for normal (Gaussian) errors while MAE is optimal of Laplacian errors. When the errors don't follow one of these patterns,there are other metrics out there that are better [@Hodson2022]. For this specific analysis, the RMSE will be used.  
A standard definition of RMSE is that it measures the average difference between a statistical model's predicted value and the actual values. Mathematically, it is the standard deviation of the residuals, which represent the distance between the regression line and the data points [@Frost]. It can be calculated by the formula:  

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$



The calculation is not complicated and with a limited number of samples, can be done in Excel. However, when the number of samples gets large, as in this analysis, R has its RMSE function to do the calculations. Our goal in this effort is to develop an algorithm that will produce a RMSE of a final_holdout_test dataset of 0.86490 or lower.


## Method
The download and data wrangling code is provided by the course and is designed to join the *ratings* 
dataset with the *movies* dataset, This code has been augmented with code that will process other datasets that are in the same general format. Functions are used within the R code to provide flexibility and the make the code easier to read.

#### Read external dataset

```{r read from datasources, echo=FALSE}
# depending of parameter selected, download working files from website or reload from disk
folder <- folders[params$dataset_selection]
dl <- urls[params$dataset_selection][[1]]
if (!file.exists(dl)) {
 file_url <- paste("https://files.grouplens.org/datasets/movielens/", dl, sep="")
 file_message <- paste('File will be downloaded from ', file_url)
 download.file(file_url, dl)
} else {
  file_message <- paste("Dataset", names[params$dataset_selection], "will be reloaded from disk")
} 

 # selection #1 has different format that the later datasets. Handle as code provided
  if (params$dataset_selection == 1) {
       ratings_file <- paste(folder, "/ratings.dat", sep="")
       if (!file.exists(ratings_file))
         unzip(dl, ratings_file)
       movies_file <- paste(folder, "/movies.dat", sep="")
       if (!file.exists(movies_file))
         unzip(dl, movies_file)
       
       ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
       colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
       ratings <- ratings %>%
         mutate(userId = as.integer(userId),
                movieId = as.integer(movieId), 
                rating = as.numeric(rating),
                timestamp = as.integer(timestamp))
    
      movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify=TRUE), stringsAsFactors=FALSE)
      colnames(movies) <- c("movieId", "title", "genres")
      movies <- movies %>% 
        mutate(movieId = as.integer(movieId))

     } else {
      unzip(dl)
      ratings <- fread(paste(folders[params$dataset_selection], "/ratings.csv", sep=""), header = TRUE)
      movies <- fread(paste(folders[params$dataset_selection], "/movies.csv", sep=""), header = TRUE)
  }

# Indicate which file was loaded and from where
print(file_message)

movielens <- left_join(ratings, movies, by = "movieId")
left_out <- anti_join(ratings, movies, by = "movieId")    #  if movies exist that are not in ratings.
```

```{r create final_holdout_test set, echo=FALSE, warning=FALSE,include=FALSE}
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead

# capture size info for complete dataset
size_movielens <- nrow(movielens)
unique_users_movielens <- length(unique(movielens$userId))

# create final_holdout_test dataset
test_index <- createDataPartition(y = movielens$rating, times = 1, p = params$testing_set_percent, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final_holdout_test set back into edx set

removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(ratings, movies, temp, removed, test_index, mmovielens)    # remove temporary data structures

# End of Provided Code

head8 <- head(edx, 8) 
nrows_edx <- nrow(edx)# printed later

edx <- dataset_prep(edx)                        # convert timestamp add columns
```

#### Data Sets
The MovieLens dataset (@Harper2015) contains `r format(nrow(movielens), big.mark= ",")` rows and `r  ncol(movielens)` columns to which two columns will be added during data wrangling. There are technically three separate datasets which are all created from the combined dataset, *movielens*.These three datasets are identified as the training set *(edx)* that contains `r format(nrows_edx, big.mark=",")` rows, the testing set *(testing*) which is used to test the training, and the final dataset *(final_holdout_test)*, which is the validation dataset. There is no duplication of rows in any of the datasets and the final is not used for any training but only to validate the algorithm on the analysis. The size of the testing and final datasets will be `r format(100 * params$testing_set_percent, digits=2)`% of the total size of *(movielens)*. This can be changed, if necessary, by modifying the parameters in the YAML header. Just in case there was a problem with generating the combined dataset, a quick check verifies that   `r ifelse (nrow(left_out) == 0, "no", format(nrow(left_out), big.mark=","))` rows from the *(movies)* dataset were omitted from the join with the *(ratings)* dataset.  
Two features were added to the training dataset: year of release and rating lag time. The year of release and subsequently the lag time are derived from the title feature. When these features are added, a check is made and `r ifelse (dels == 0, "no", format(dels, big.mark=','))` titles that do not fit this pattern were found `r ifelse (dels==0, ".", "and will be deleted since the release year cannot be determined.")`


#### List of features

After the movies and ratings set have been combined, any "dirty" rows removed, and any other data wrangling performed, the first eight rows of the dataset that will be used for training loos like:
```{r edx layout, echo=FALSE, results="asis"}
# show first 8 rows of the edx dataset
head8 %>% 
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10, full_width=FALSE, position="center", latex_options = "hold_position", "striped")

column_df <- data.frame(feature = colnames(edx),
                        class = c("integer", "numeric", "numeric", "integer", "character", "character", "numeric", "date"),
                        description = c("unique ID for each movie", "unique ID for each user", "rating (0.5-5) that a user gave a movie",
                                        "timestamp as to when the user rated the movie", "title of the movie and year of release", 
                                        "the genre or genres assigned to the movie", "time between the release date and the date the movie was rated by the user",
                                        "the year of release")
)                                        
```
```{r feature tables, echo=FALSE, fig.align = "center"}
column_df %>% 
  kbl(caption = "Description of Features") %>% 
  kable_styling(bootstrap_options = c("striped", "condensed", "hover"), full_width = FALSE, position="center", latex_options = "hold_position") 

``` 
```{r,necessary calculations for plotting, echo=FALSE, warning=FALSE, include=FALSE}
# ------------------------------------------------------------------------------------------------------
# code for calculating and producing the various graphs (each plot is a separate chunk)
# ------------------------------------------------------------------------------------------------------

df <- edx %>% ungroup() %>% group_by(releaseYear) %>% reframe(n=n())                    # extract year from max ratings count
year <- format(as.Date(edx[which.max(df$n),]$releaseYear, format("%Y-%m-%d")), "%Y")
sum_df <- sum(df$n)

df1 <- edx %>% ungroup() %>% mutate(rater_year = as.integer(str_extract(timestamp, "\\d{4}"))) %>%  group_by(rater_year) 
min_rater <- min(df1$rater_year)
max_rater <- max(df1$rater_year)
df1 %>% group_by(userId) %>% summarize(mx = max(timestamp), mn = min(timestamp)) %>% mutate(diff = difftime(mx, mn, units="days"))
df2 <- df1 %>% group_by(userId) %>% summarize(mx = max(timestamp), mn = min(timestamp)) %>% mutate(diff = difftime(mx, mn, units="days"))
df3 <- df1 %>% group_by(rater_year) %>% reframe(n = n())

set.seed(1, sample.kind = "Rounding")
small_edx <- edx %>% ungroup() %>% slice_sample(n=2000)

single_rating <- edx %>%
  group_by(userId) %>% 
  reframe(n_distinct(userId),diff = max(rating) - min(rating) == 0, rating) %>% filter(diff == 1)  %>% 
  group_by(userId, rating) %>% summarize(cnt = n()) %>% arrange(rating) %>% group_by(rating, cnt) %>%  
  ungroup %>% mutate(unit = 1) %>% group_by(userId) %>% 
  reframe(rating, cnt, unit) %>% arrange(rating) %>% reframe(n=n(), rating, Ratings= sum(cnt), Users=sum(unit), .by=rating) %>%
  group_by(rating) %>% reframe(rating, Users, Ratings) %>% unique()

df_average_ratings <- edx %>%
        group_by(userId)%>%
        summarize(avg = mean(rating)) %>%
        arrange(desc(avg))

```

```{r plot number of raters per year, echo=FALSE, warning = FALSE, include=FALSE}
number_of_raters_plot_per_year <- df1 %>%
  ggplot(aes(rater_year)) + 
  geom_bar(stat="count",bins = 30) + 
  xlab("Year") +
  ylab("Number of Ratings") +
  scale_y_continuous(labels = function(x) format(x, big.mark = ",", scientific=FALSE)) +
  ggtitle("Number of Raters by Year") +
  theme(plot.title = element_text(hjust = 0.5))
```
```{r plot number of ratings  per year, echo=FALSE, warning = FALSE, include=FALSE}
number_of_ratings_per_rating_year_plot <- df3 %>%
  ggplot(aes(x = rater_year, y = n)) + 
  geom_bar(stat = "identity", bins=30 ) + 
  xlab("Rating Year") +
  ylab("Number of Ratings") +
  scale_y_continuous(labels = function(x) format(x, big.mark = ",", scientific=FALSE)) +
  ggtitle("Nbr of Ratings by Rating Year") +
  theme(plot.title = element_text(hjust = 0.5))
number_of_ratings_per_rating_year_plot 
```

```{r plot length of presence, echo=FALSE, warning=FALSE, include = FALSE}
rater_presence_plot <- df2 %>% select(diff) %>% ggplot(aes(diff)) + 
                         geom_histogram() + 
                         xlim(-1, 5*365) + 
                         ylim(0, 4000) +
                         xlab("Days") +
                         ylab("Number of Raters") +
                         ggtitle("Days Between 1st and Last Rating") +
                         geom_vline(xintercept = c(365,2*365, 3*365, 4*365), color="blue")  +    
                         theme(plot.title = element_text(hjust = 0.5))


```

```{r plot number of ratings per year, echo=FALSE, fig.align="center", fig.width = 6.75, fig.height = 3.75, fig.show = "hold"}
# plot number of ratings per year
number_of_ratings_per_release_year_plot <- edx %>% select(userId, movieId, releaseYear) %>% mutate(yr = year(releaseYear)) %>% 
  ggplot(aes(yr)) + 
  geom_bar(stat="count") + 
  scale_y_continuous(labels = function(x) format(x, big.mark=",", scientific=FALSE)) +
  xlab("Release Year") +
  ylab("Number of Ratings") +
  ggtitle("Nbr of Ratings by Release Year")
  
```

```{r, distribution of ratings plot, echo=FALSE}
# plot distribution of ratings plot
distribution_of_ratings_plot <- edx %>% select(movieId, rating) %>%
         ggplot(aes(rating)) +
         geom_histogram(stat = "count", bins=30, binwidth=0.3, color = "black") +
         scale_y_continuous(labels = function(x) format(x, big.mark = ",", scientific=FALSE)) +
         ggtitle("Distribution of Ratings")
```
```{r, number of ratings per movie, echo=FALSE}
# number of ratings per movie
number_of_ratings_per_movie_plot <-edx %>%
         count(movieId) %>%
         ggplot(aes(n)) +
         geom_histogram(bins=30, color="black") +
         scale_x_log10(labels = function(x) format(x, big.mark = ",", scientific=FALSE)) +  
         scale_y_continuous(labels = function(x) format(x, big.mark = ",", scientific=FALSE)) +
         xlab("number of ratings") +
         ylab("number of movies") +
         ggtitle("Number of Ratings per Movie")
```


```{r graph of movie age vs rating lag, echo=FALSE, warning=FALSE, , fig.width = 3.75, fig.height=3.75, fig.align = "center"}
# plot of movie age vs rating lag
movie_age_against_lagtime_plot <- small_edx %>%
  ggplot(aes(releaseYear, ratingLag, group=releaseYear)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "black", aes(group = 1)) +
    xlab("Year of Release") +
    ylab("Days Between Release and Rating") +
    ggtitle("Movie Age vs Rating Lag") 
```

```{r Frequency of Average Ratings given by users graph, echo=FALSE, warning=FALSE}
# plot of frequency of average ratings given by users 
frequency_of_avg_ratings_plot  <- df_average_ratings %>%  ggplot() +
    geom_histogram(aes(avg), bins = 10, binwidth = 0.25) +
    xlab("Average Rating per User") +
    ylab("Frequency of Rating") +
    ggtitle("Frequency of Average Ratings \nGiven By Users")

```
```{r min/max for a user, echo=FALSE, message=FALSE, warning=FALSE}
# plot of the minimum and maximum ratings given by a user
min_max_per_user_plot1 <- edx %>%
  group_by(userId, rating) %>%
  summarize(num = n()) %>%
  group_by(userId) %>%
  summarize(distinct = n_distinct(rating)) %>%
  ggplot() +
    aes(distinct) +
    geom_histogram(binwidth=1) +
    xlab("Number of Distinct Ratings") +
    ylab("Number of Users") +
    ggtitle("Distinct Ratings Per User") 
```    

```{r different ratings for a user, echo=FALSE, warning=FALSE, message=FALSE}
# does the rater give multiple ratings? Plot of min/max ratings given by a specific user.
# How many different ratings does each user give           
min_max_per_user_plot2 <- edx  %>%
  group_by(userId) %>%
  summarize(min = min(rating), max = max(rating)) %>%
  ggplot() +
    geom_histogram(aes(min), binwidth=0.5, fill="blue", alpha = 0.5) +
    geom_histogram(aes(max), binwidth=0.5, fill="red", alpha=0.5) +
    xlab("Rating") +
    ylab("# of Users") +
    ggtitle("Min/Max Per User")
```

```{r spread of user ratings,echo =FALSE, message=FALSE, warning=FALSE}
# plot of the spread of ratings given by a particular user
spread_of_user_ratings_plot <- edx %>% 
  group_by(userId) %>%
  summarize(spread = max(rating) - min(rating))%>%
  ggplot(aes(spread)) +
    geom_histogram(binwidth = 0.5) +
    xlab("Number of Distinct Ratings") +
    ylab("Number of Users") +
    ggtitle("Min/Max Ratings Spread")

```
```{r single rate for graph, echo=FALSE, message=FALSE, warning=FALSE, include=TRUE}
# Does a user only give one rating? Plot of those who do.
single_rate_plot <- single_rating %>%
  ggplot() +
  geom_col(aes(x = rating, y = Users)) +
  ggtitle("Single Rating by User") +
  xlab("Rating Given") +
  ylab("Number of Users") 
```
```{r, expand dataset for genres, echo=FALSE}
# expands dataset by separating the genres column value into separate rows
    edx_genres <- expand_dataset(edx)
  
# time-safer option -- polished dataset saved to disk
if (params$save_files) {
  saveRDS(edx, file = "edx.rds")
  saveRDS(final_holdout_test, file = "final_holdout_test.rds")
}

```


## Exploratory Data Analysis
Exploratory Data Analysis (EDA) is not a formal process with a strict set rules but rather a state of mind where the analyst should investigate every idea that pops up. Some will work out; some will be duds, but it is important part of any data analysis because you always need to investigate the quality of the data [@Wickham2023]. For this analysis,there are three components of the investigation: the movies, the users, and the ratings.  
We start visualizing our EDA by first looking at the raters, a.k.a. users, of this data. The users were selected at random and there is no demographic information about a rater other than the user ID, so we are left with the user ID and their actions. In this dataset, there are `r format(unique_users_movielens, big.mark=",")` unique raters that have made `r format(size_movielens, big.mark=",")` ratings in the period of `r min_rater` to `r max_rater`. The following graphs show the number of raters the participated by year and the length of time that a user remains active in the rating process  from a minimum of `r min(df2$diff)` days (`r format(nrow(df2[df2$mx == df2$mn,]), big.mark=",")` raters have this duration) to a maximum of `r format(max(df2$diff), big.mark = ",")`(approximately `r format( as.integer(max(df2$diff/365.25),digits=4))` years). 

```{r rater plot, echo=FALSE, warning=FALSE, include=FALSE, fig.height=3.5}
grid.arrange(number_of_raters_plot_per_year,rater_presence_plot, ncol = 2)
```
  
We next look at how these ratings were spread out over the years. The two graphs that follow show the number ratings by the year the movie was released and the 
number of ratings by calendar year.  

```{r, time lapse of ratings, echo=FALSE, fig.height=3.5}
grid.arrange(number_of_ratings_per_release_year_plot, number_of_ratings_per_rating_year_plot, ncol = 2, heights=c(4,4))
```
  
Now we examine how these ratings are applied to the various movies. The movies are rated on a scale between 0 and 5 in 0.5 increments. The first graph shows the distribution of the rating values. The distribution is not normal by any means and the graph reveals that using a Â½ rating was not a prevalent as a full integer rating. Not all movies received the same number of ratings, but the number of movies that did receive a rating as shown in the second graph indicates that there is sufficient data to proceed with the analysis.
```{r, number of ratings, echo = FALSE, fig.height=3.5}
grid.arrange(distribution_of_ratings_plot, number_of_ratings_per_movie_plot, ncol = 2)
```
\newpage
The next two graphs show how a particular rater would rate a movie. When a rater gives only a single rating, the first graph shows the distribution of that rating. This could affect the objectivity of the rating. To look at this somewhat closer we find that there are a total of `r format(sum(single_rating$Users), big.mark=",")` which represents `r format(sum(single_rating$Users * 100 / nrows_edx), scientific = FALSE, digits=5)`%, which is a rather insignificant component and will be ignored. When a user gave different ratings, of the ten possible ratings, how many were used. The second graph shows this distribution. 

```{r rating spread, echo=FALSE, fig.height=3.5}
grid.arrange(single_rate_plot, min_max_per_user_plot1, ncol = 2)
```
  
The first plot below shows how a particular rater spread the ratings that were given. The second graph shows for those users that did not give a single rating how many of the ten (0 - 5) ratings were used. 

```{r rating distribution, echo=FALSE, fig.height=3.5}
grid.arrange(min_max_per_user_plot2, spread_of_user_ratings_plot,  ncol=2)
```

We've seen what the distribution of ratings over all users, but we can split that down further by looking at the average distribution by an individual user as shown in the first graph below. Another correlation that needs be investigated is that rating lag, that is, the time between the release of the movie and the time the movie was rated by a rated. That correlation is shown in the second graph and shows that rating lag is not a significant feature to be analyzed.  

```{r, miscellaneous plots, echo=FALSE, warning=FALSE, message=FALSE, fig.height=3.5}
grid.arrange(frequency_of_avg_ratings_plot, movie_age_against_lagtime_plot, ncol = 2)
```
  
There are 19 genres that can be assign to a movie and a movie can have one or more genres assigned to it. The assignment of genre is usually done by the studio and/or writers and is determined by the literary technique, tone, content and some times by length [@William2024]. A break down of the movies that are in each genre is below.
```{r movies by genre, echo=FALSE, fig.height=4, fig.width=7, fig.align="center", fig.fullwidth=TRUE}
# plot the number of movies by genre. One movies may have multiple genres
if (params$expand_genres) {
  edx_genres  %>%
    select(genres, rating) %>%
    group_by(genres) %>%
    ggplot(aes(x = factor(genres))) +
    geom_bar(stat="count") +
    ggtitle("Movie Ratings by Genre") +
    xlab("Genre") +
    ylab("Rating") +
    scale_y_continuous(labels = function(x) format(x, big.mark=",",  scientific=FALSE)) +
    theme(axis.text.x = element_text(angle=90, vjust=1, hjust=1))
    } else {
    edx %>% group_by(genres) %>% reframe( nn = n()) %>% slice_max(order_by=nn, n=25) %>% arrange(genres) %>%
    ggplot(aes(x = factor(genres), y = nn)) +
    geom_bar(stat="identity") +
    xlab("Genre") +
    ylab("Number of Ratings") +
    ggtitle("Movie Ratings by Genre Group") +
    scale_y_continuous(labels = function(x) format(x, big.mark = ",", scientific = FALSE)) +
    theme(axis.text.x  = element_text(angle=90, vjust=1, hjust=1))
}

```

Since each movie can have multiple genres, the dataset needs to be expanded so that there is only one genre per rating. This is only to determine the effect of the genre feature on the analysis; for all other features the unexpanded dataset is used.


We now need to examine the data for any anomalies that might cause us problems later. We want to work with tidy data so we need to verify that the training dataset, *edx*, is tidy. To be tidy, three conditions must be met:

* Each variable must have its own column
* Each observation must have its own row
* Each value must have its own cell


#### Preliminary review
```{r earliest movie info, echo=FALSE}
# get data for the first movie in the dataset
min_year <- substr(min(edx$releaseYear),1,4)
min_title <- edx[which.min(edx$releaseYear),]$title
# some movie titles have "The" at the end. Move this to the start for aesthetics
min_title <- substr(min_title, 1, nchar(min_title)-7)
if (endsWith(min_title, ", The")) {
  min_title <- paste("The", substr(min_title, 1, nchar(min_title)-5))
}

```

The earliest movie in the dataset is `r min_title` that was released in `r min_year` and the 
maximum number of ratings (`r format(sum_df, big.mark = ",")`  ) occurred in the year `r year`.


```{r obscure movie calculations, echo=FALSE, message=FALSE}
# table of obscure movies. User has option of excluding these or not from the dataset
obscure_movies <- edx %>% 
  group_by(movieId) %>% 
  reframe(count = n()) %>%
  filter(count == 1) %>%
  left_join(edx, by = "movieId") %>%
  group_by(title) %>%
  reframe(movieId, rating = rating, n_rating =  count) 

```
 Included in data set are some obscure movies that could be considered to be removed from the majority of movies. There are `r nrow(obscure_movies) ` 
 (`r format(100*nrow(obscure_movies) / nrow(edx), digits = 2)`% of the working dataset)  that have only a single rating in the dataset. 
 A table of 15 of these movies and the statistics on their ratings follows:

```{r table of obscure movies, echo=FALSE, warning=FALSE}
obscure_movies %>% select(-movieId) %>% slice(1:15) %>%
  kable() %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 10)
```
```{r removal of obscure movies - uncomment to implement, echo=FALSE, warning=FALSE}
# If desired, obscure movies are removed
if (params$remove_obscure) {
  nrows1 = nrow(edx)
  edx <- edx[!edx$movieId %in% obscure_movies$movieId,]
}
```
The summary stsatistics for these obscure movies is:  
```{r table of average rating vs mean, echo=FALSE }
# create table of average rating vs mean
edx %>%
  group_by(userId) %>%
  summarize(Avg_Rating = mean(rating))  %>%
  select(Avg_Rating)%>%
  summary(Avg_Rating) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), font_size = 12)
```
Removing these obscure movies reduces the number of rows in the training dataset to `r format(nrow(edx), big.mark=",")` rows.

```{r build testing and training data sets, echo=FALSE, message=FALSE, warning=FALSE}
# Split the edx dataset into a testing and a training data set. Percentage is determined by knit parameter
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(edx$rating, times = 1, p =     params$subset_percent, list = FALSE)
training_set <- data.frame(edx[-test_index,])
temp <- data.frame(edx[test_index ,])
testing_set <- temp %>%
  semi_join(training_set, by = "movieId") %>%
  semi_join(training_set, by = "userId")

# add back missing rows to the training set and remove extraneous data
withheld <- anti_join(temp, testing_set)
training_set <- rbind(training_set, withheld)
rm (temp, withheld)
```

```{r initialize results, echo=FALSE}
# create data frame for holding the results of the calculations
rmse_results <-data.frame(matrix(nrow=0, ncol=3))
colnames(rmse_results) <- c("Method", "RMSE", "Difference")

```
```{r comment}
# ------------------------------------------------------------------------------------------------------
# Modeling
# ------------------------------------------------------------------------------------------------------
```
## Modeling
Data modeling refers to the process of mapping data at typically three levels: conceptual, logical, and physical. It creates visual maps and references that allow data analysts to visualize a data system and is a process by which data systems can be interconnected [@Pierson2024]. We will
work several models that pertain to features that could influence the recommendation.

#### Model 0 - Naive model
This is a simple, naive  models which is predicting average movie rating for all observations. The formula  for this model is simply\

$$ Y_{} = \mu $$
```{r, zero bias, echo = TRUE, include=TRUE}
mu <- mean(training_set$rating)
zero_rmse <- RMSE(testing_set$rating, mu)
zero_rmse

```
```{r zero_bias logging, echo=FALSE}
# save results
rmse_results <- rbind(rmse_results, c("No biases applied - Median Alone", zero_rmse, 0))

```
---- ----------------------------------------------------------------------------------------------------------------------------

#### Model 1 - Average movie rating model

The formula for this model is\
$$ Y_{} = \mu + b.movie_i $$
where $\mu$ is the mean of the data set (model 0), and $b.movie_i$ is an error term that describes the random variability.To improve this RMSE, we will consider adding any effects that the movie may have on this value. There was a hint of this effect when we noticed the more ratings that a movie receives, the higher they often rated. We introduce a movie bias term, $b_{movie}$, which is the difference between the average of a specific movie$_i$ and the average for all movies.
```{r movie bias calculation, echo = FALSE}
# Model 1 -- movie bias
movie_bias_df <- training_set %>% 
              group_by(movieId) %>% 
              summarize(bm = mean(rating - mu))
```


                  
#### RMSE Movie Bias Calculation
We can now predict the rating with $\mu$, and we can obtain the RMSE for Model 1:
```{r movie bias prediction and RMSE calculation, echo=TRUE}
predicted_ratings <- mu + testing_set %>% 
                     left_join(movie_bias_df, by = "movieId") %>%
                     .$bm
movie_bias_rmse <- RMSE(predicted_ratings, testing_set$rating)
movie_bias_rmse
```

```{r amovie bias rmse logging, echo=FALSE}
# save RMSE results
rmse_results <- rbind(rmse_results, c("Median + Movie Effects", movie_bias_rmse, 0))


```

---- ----------------------------------------------------------------------------------------------------------------------------

#### Model 2 - Model 1 with user effect model
We've got a RMSE for the average movie rating, but it is over our objective is to be below 0.86490, so we need to continue. We have noticed that a group of users the consistently give low ratings and there are those that consistently give higher ratings. Will use the symbol $b.user_u$ as this bias for user *u*. 
$$Y_{i} = \mu + b.movie_i + b.user_i$$ \

```{r user bias calculation, echo= FALSE}
# Model 2 -- Model1 with user effects
user_bias_df <- training_set %>% 
               left_join(movie_bias_df, by = "movieId") %>%
               group_by(userId) %>% 
               summarize(bu = mean(rating - mu - bm))
```
##### RMSE User Bias Calculations
```{r user bias prediction and rmse calculations, echo=TRUE}
predicted_ratings <- testing_set %>% 
                     left_join(movie_bias_df, by='movieId') %>% 
                     left_join(user_bias_df, by="userId") %>%
                     mutate(pred = mu + bm + bu) %>% 
                     .$pred
user_bias_rmse <- RMSE(predicted_ratings, testing_set$rating)
user_bias_rmse
```
Graphs for both the movie bias and user bias are shown below.

```{r movie bias graph, echo = FALSE}
movie_bias_graph <- ggplot(movie_bias_df, aes(x = bm)) +
    geom_histogram(bins=30, fill="yellow", color="lightgray") +
    xlab("Movie Bias") +
    ylab("Number of Movies") +
    ggtitle("Distribution of Movie Bias") +
    theme_linedraw()
```
```{r user bias graph, echo=FALSE}
user_bias_graph<- ggplot(user_bias_df, aes(x = bu)) +
    geom_histogram(bins=30, fill="yellow", color="lightgray") +
    xlab("User Bias") +
    ylab("Number of Movies") +
    ggtitle("Distribution of User Bias") +
    theme_linedraw()

grid.arrange(movie_bias_graph, user_bias_graph, ncol=2, padding=20) 
```


```{r movie bias logging, echo=FALSE}
# save movie bias RMSE results
rmse_results <- rbind(rmse_results, c("Median + Movie + User Effects", user_bias_rmse, 0))

```

The RMSE is looking better. It is below 1 but still above our goal. Another model is in order.  
---- ----------------------------------------------------------------------------------------------------------------------------

#### Model 3 - Model 2 with genres effect model
To the movie 2 model, we will add a genres effect term.The formula now becomes:\
$$Y_{u,i} = \mu + b.movie_i + b.user_u + b.genre_{u,i}$$ \

```{r genres bias calculations, echo=FALSE}
# Model 3 -- Model 2 plus genre -- for this calculation the expanded data set is used, if selected
genres_bias_df <- training_set %>% 
              left_join(movie_bias_df, by="movieId") %>%
              left_join(user_bias_df, by='userId')  %>% 
              group_by(genres) %>% 
              summarize(bg = mean(rating - mu - bm - bu))
```


#### RMSE Genres Bias Calculation
```{r calculate genres effects, echo=TRUE}
predicted_ratings <- testing_set %>% 
                      left_join(movie_bias_df, by="movieId") %>%
                      left_join(user_bias_df, by='userId')  %>%
                      left_join(genres_bias_df, by="genres") %>%
                      mutate(pred = mu + bu + bm + bg) %>% 
                      .$pred
genres_bias_rmse <- RMSE(predicted_ratings, testing_set$rating)
genres_bias_rmse
```


```{r add genres bias results into rmse_results, echo=FALSE}
# save genres bias RMSE results
rmse_results <- rbind(rmse_results, c("Median + Movie + User + Genres Effects", genres_bias_rmse, 0))

```
---- ----------------------------------------------------------------------------------------------------------------------------

#### Model 4 -- Model 3 with year of release bias

The graph for the distribution of the year of release bias on the data and indicates that some improvement would be made if this bias was included in the 
analysis.  
```{r release year bias calculation, echo=FALSE}
# Model 4 -- Model 3 with release year bias
year_bias_df <- training_set %>% 
              left_join(movie_bias_df, by="movieId") %>%
              left_join(user_bias_df, by='userId') %>% 
              left_join(genres_bias_df, by="genres") %>%
              group_by(releaseYear) %>% 
              summarize(by = mean(rating - mu - bm - bu - bg))
```
##### RMSE Year Released Bias Calculations

```{r release year prediction and RMSE calculations, echo = TRUE}
predicted_ratings <- testing_set %>% 
                     left_join(movie_bias_df, by="movieId") %>%
                     left_join(user_bias_df, by='userId') %>%
                     left_join(genres_bias_df, by="genres") %>%
                     left_join(year_bias_df, by = "releaseYear") %>%
                     mutate(pred = mu + bu + bm + bg + by) %>% 
                     .$pred

releaseYear_rmse <- RMSE(predicted_ratings, testing_set$rating)
releaseYear_rmse
```

```{r release year rmse logging, echo=FALSE}
# save release year RMSE results
rmse_results <- rbind(rmse_results, c("Median + Movie + User + Genres Effects + Release Year  ", releaseYear_rmse, 0))
```

The graphs for the genres and release year biases look different than the previous graphs.

```{r genres bias graph, echo=FALSE}
genres_bias_graph <- ggplot(genres_bias_df, aes(x = bg)) +
    geom_histogram(bins=30, fill="yellow", color="lightgray") +
    xlab("Genres Bias") +
    ylab("Number of Movies") + 
    ggtitle("Distribution of Genres Bias") +
    theme_linedraw()

```


```{r release year graph, echo=FALSE}
year_bias_graph <-ggplot(year_bias_df, aes(x = by)) +
    geom_histogram(bins=30, fill="yellow", color="lightgray") +
    xlab("Year of Release  Bias") +
    ylab("Number of Movies") +
    ggtitle("Distribution of Year of Release Bias") +
    theme_linedraw()

grid.arrange(genres_bias_graph, year_bias_graph, ncol=2, padding=20)
```
---- ----------------------------------------------------------------------------------------------------------------------------

### Regularization of Model 4
This is better than before but still above the goal of 0.84690. To bring that value down below the goal, we will apply regularization that adds constraints to total variability of the various effects. Regularization is a set of methods for reducing over fitting in models such as this one. Typically, regularization exchanges a marginal decrease in training accuracy for an increase in generalizability [@Muriel2023]. We implement regularization by include a new term lambda, $\lambda$ into our equation which now becomes\
$$\frac{1}{N} \sum_{u,i} (y_{u,i} = \mu - b.movie_i - b.user_u - b.genres_{u,i})^2 + \lambda (\sum_{i} + b.movie_i^2 + \sum_{i} b.user_{u}^2)$$
where $b.movie_i$ is influenced by movies that have jut a few ratings. $b.user_u$ is influenced by those users who only rated a small number of movies. $b.genres_{u,i}$ is influenced by the genres that are not often associated with a movie such as IMAX. The use of regularization allows us to penalize these effects. Consequently, we can use $\lambda$ as a tuning parameter that, by adjusting, allows us to minimize the RMSE. A range of $\lambda$'s were evaluated to determine witch value produced by lowest accuracy. The graph of these tests is shown below.
```{r regularization, echo=FALSE}
# regularization of Model 4
lambdas <- c(seq(-0.75, 2.5, 0.25))                                 # range of lambdas for testing
rmses <- sapply(lambdas, regularization_function, testing_set)   # test of each lambda specified
min_lambda <- lambdas[which.min(rmses)]                          # select lowest value (best accuracy)
```

```{r lambda plot rmse results, echo=FALSE, warning=FALSE, fig.align="center", fig.width=6, fig.height=4}
# visualizing where lambda minimizes RMSE
plot_rmses <- ggplot() + 
              aes(x = lambdas, y = rmses) + 
              geom_point() + xlab("Lambda") + 
              ylab("RMSE") + ggtitle("Lambda Tuning") +
              theme(plot.title = element_text(hjust = 0.5)) +
              geom_vline(xintercept = min_lambda, color="blue")
plot_rmses
```
```{r print minimum lambda, echo = FALSE}
# print the result of the calculations
message(paste("Minimum lambda: ", min_lambda))

```

```{r regularization rmse calculation, echo=FALSE}
regularization_rmse <- regularization_function(min_lambda, testing_set)
regularization_rmse

```

```{r regularization rmse logging, echo=FALSE}
# save regularization RMSE results
rmse_results <- rbind(rmse_results, c("Median + Movie + User Effects + regularization", regularization_rmse, 0))

```

                     

## Results  
Now that we have trained the algorithm with the training dataset, it is time to validate it. For this we use the final_holdout_test dataset which has not been used up to this point. We apply the same preparation to this dataset as was applied to the training dataset, i.e., converting the timestamp, adding rating lag and year release, removing obscure movies, and expanding the genres column. We then apply the training algorithm that was developed.
```{r prep final_holdout_test data set, echo=FALSE, message=TRUE, include=FALSE}
# perform same operations on final_holdout_test dataset as was done on edx
final_holdout_test <- dataset_prep(final_holdout_test)
if (params$remove_obscure) {
  final_holdout_test <- final_holdout_test[!final_holdout_test$movieId %in% obscure_movies$movieId,]
}
final_holdout_test <- expand_dataset(final_holdout_test)

final_holdout_test_rmse <- regularization_function(min_lambda, final_holdout_test)
final_holdout_test_rmse

# save final_holdout_test RMSE results
rmse_results <- rbind(rmse_results, c("final_holdout_test", final_holdout_test_rmse, 0))
```



```{r create and display summary table, echo=FALSE, warning=FALSE}
# show results of this analysis
colnames(rmse_results) <- c("method", "rmse", "change")

rmse_results %>%
  mutate(change = format(as.numeric(rmse) - first(as.numeric(rmse), digits=4)), rmse = format(as.numeric(rmse), digits=5)) %>% 
  mutate(rmse = cell_spec(rmse, background = ifelse(rmse <= 0.8469, '#90EE90', "white"))) %>% 
  knitr::kable(col.names = c('Method', 'RMSE', 'Change'), align = "lcr", "simple", 
               caption = "Summary of Model RMSEs (goal: 0.86490)") %>%
  kable_styling( )


```
Using a $\lambda$ of `r format(min_lambda, digits=3)`, we determine a rmse of `r format(regularization_rmse, digits=6)` for the training set. It is now time for the grandiose test -- our algorithm against the final_holdout_test dataset. Using the final_holdout_test dataset, the rmse for that dataset is `r format(final_holdout_test_rmse, digits=6)`. The goal has been met and exceeded.




#### Other tests
An analysis of the supplied dataset with the obscure movies removed modified is complete. Other different, but similar, datasets are available from the grouplens website, https://grouplens.org/datasets/movielens/ which contain more data or are more recent. The analysis described here was used to analyze some of those datasets and the results are published here:
```{r test results summary, echo=FALSE, fig.align="center", fig.width=6, fig.align="center"}
# Tests of other datasets from movielens
test_dataset <-   c("ml-10M100K", "ml-10M100K", "ml-32M-download", "ml-32M-download", "ml-latest", "ml-latest", "ml-latest-small", "ml-latest-small")
test_size    <-   c("8,999,929",  "10,000,054", "28,749,912",      "TBD",             "33,832,162","30,419,985","87,436",         "91,112"          )
test_obscure <-   c("Yes",        "No",         "Yes",             "No",              "Yes",       "No",        "Yes",             "No"             )
test_training <-  c(0.82029,      0.81992,      0.83488,           0.81598,           0.80875,     0.80875,     0.65521,           0.65892          )
test_final_holdout_test <-c(0.81796,      0.81796,      0.81451,           0.81446,           0.80921,     0.80921,     0.68537,           0.67877          )
test_date    <-   c("2024-10-22", "2024-10-23", "w024-10-23",      "2024-10-21",      "2024-10-21","2024-10-23","2024-10-22",      "2024-10-22"     )
test_time    <-   c("6.20 mins",  "5.95 mins",  "25.89 mins",      "24.97 mins?",     "30.00 mins","34.51 mins","0.12 mins",      "0.12 mins"       )
test_results <- data.frame(test_dataset, test_size, test_obscure, test_training, test_final_holdout_test,
                           test_date, test_time)
test_results %>% knitr::kable(col.names=c("Dataset", "Rows", "Obscure Removed", "Training Set", "final_holdout_test Set", "Date", "Time"), 
                              align="llllllll", "simple", position="center")
# question mark in time indicates run all as opposed to knit
```

## Conclusion
For this project, we built a machine learning model that used the MovieLens Dataset to forecast movie ratings that takes into account any user movie bias, user bias, or genres bias and release year bias. The goal was to obtain an accuracy of 0.86490 or lower, which was accomplished. 

```{r}

```

\newpage
## Appendix
#### System Information
RStudio: version 2024.09.0 Build 375  
R: version 4.3.2 (2023-10-21 ucrt) -- "Eye Holes"    
Windows 11 Pro Version 22H2 OS Build 22621.4317 64 bit operating system 64 bit processor    
Dell 5431 Processor 2.20 GHz Memory 32 GB    

```{r show parameters, echo=FALSE, fig.align = "center", results="asis" }
show_parameters()  %>%
    kbl(caption = "Parameters for This Analysis") %>%
    kable_styling(full_width = FALSE, latex_options = "hold_position", position = "left")
```  

```{r stop time, echo=FALSE}
# Show how long this analysis took
print(paste('Elapsed time for this analysis:', format(as.numeric(difftime(Sys.time(), start_time, units="mins")), digits=5), "mins"))

```
    
\newpage 
## Bibliography